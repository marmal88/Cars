{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PhbvEzKuyC-e"
      },
      "source": [
        "# Car Identification Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Zix_S7k318"
      },
      "source": [
        "# Preliminary EDA from metadata files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sCDWF1Ek319"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DgtAryUk31-"
      },
      "outputs": [],
      "source": [
        "names_df = pd.read_csv('data/annotations/class_names.csv', header=None)\n",
        "names_df.rename(columns={0:\"class_names\"}, inplace=True)\n",
        "names_df.index = (np.arange(1, len(names_df) + 1))\n",
        "\n",
        "data_df = pd.read_csv('data/annotations/cars_annos.csv', sep=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "X8MIcGXAk31_",
        "outputId": "b8bf9aa3-6b7b-462a-db41-e8848fb489dc"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(data_df, names_df, how='inner', left_on=\"class\", right_index=True)\n",
        "col = {x:x.lower() for x in df.columns}\n",
        "df[\"class\"] = df[\"class\"].apply(lambda x: x-1)\n",
        "df.rename(columns=col, inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JilQXHaLk32B",
        "outputId": "ba0fe7f7-8da2-4116-9f2c-978b60112b71"
      },
      "outputs": [],
      "source": [
        "print(f\"The minimum in class is {df['class'].min()} and the maximum in class is {df['class'].max()}\")\n",
        "# print(df[\"class\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Rw2sxgk32C",
        "outputId": "d4eea4ac-c5b9-49af-c572-f445d5212f28"
      },
      "outputs": [],
      "source": [
        "# Check to see if the dataset split is already fair by ensuring stratification of classes over the train and test set\n",
        "test_df = df.loc[df[\"test\"]==1]\n",
        "train_df = df.loc[df[\"test\"]==0]\n",
        "\n",
        "print(f\"total number of unique classes entire dataset {df['class'].nunique()}\")\n",
        "print(f'total number of unique classes in train_set is {test_df[\"class\"].nunique()}')\n",
        "print(f'total number of unique classes in test_set is {train_df[\"class\"].nunique()}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saving out the training and testing dataframes\n",
        "\n",
        "- training dataframe\n",
        "  - to be further split into training and validation dataset for training\n",
        "- testing dataframe\n",
        "  - to be kept as a hold out set for final testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_df.to_csv(\"data/annotations/train_df.csv\", index=False) # \"data/annotations/train_df.csv\"\n",
        "test_df.to_csv(\"data/annotations/test_df.csv\", index=False)  # \"data/annotations/test_df.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "6-Vfnu37k32E",
        "outputId": "5cbca68d-6417-409f-e96b-6a1ab0ec67d8"
      },
      "outputs": [],
      "source": [
        "# Random check to see if bounding boxes are able to completely cover the car in question\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(5):\n",
        "    rand = np.random.randint(len(df))\n",
        "    img_name, x1, y1, x2, y2, _, _, class_names = df.iloc[rand,:]\n",
        "\n",
        "    width = x2-x1\n",
        "    height = y2-y1\n",
        "    print(img_name, x1, y1, x2, y2, width, height, class_names)\n",
        "    \n",
        "    img = Image.open(f'data/car_ims/{img_name}')\n",
        "    ax=plt.subplot(1,5,i+1)\n",
        "    plt.imshow(img)\n",
        "    rect = Rectangle((x1, y1), width, height, linewidth=1.5, edgecolor='r', facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "    plt.title(f\"{class_names}\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C5Bmt0Rk32H",
        "outputId": "e09055c5-a7fe-4ac2-8d97-85d0b33f6c81"
      },
      "outputs": [],
      "source": [
        "# Check to see if there are different modes in the dataset RGB vs RGBA, seems like all are jpg\n",
        "df[\"image\"].apply(lambda x: os.path.splitext(x)[1]).unique()\n",
        "# In retrospect, this check was not sufficient as there were several images that had only one channel. \n",
        "# Alternate method was to use os.walk to return a list of metadata from the Images themselves"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz9A7W2DyGw_"
      },
      "source": [
        "# Model Training\n",
        "This section shows the experimentation stage for creation of dataset, dataloader, model, train and evalution loops.\n",
        "\n",
        "The actual model and training was conducted using the .py files. \n",
        "\n",
        "While all effort has been taken to ensure consistency. In the event of discrepency please refer to .py file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GPzYao2ByCnX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision.models import resnet50, ResNet50_Weights, resnet101, ResNet101_Weights\n",
        "from PIL import Image\n",
        "from typing import Dict\n",
        "from tqdm import tqdm \n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from src.config.load_config import read_yaml_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw9VAKsSk32J",
        "outputId": "649bb7a9-c91a-46a2-debb-fe3524296400"
      },
      "outputs": [],
      "source": [
        "cfg = read_yaml_file()\n",
        "\n",
        "# Loading of training dataset\n",
        "train_df_path = cfg[\"training\"][\"train_df_path\"]\n",
        "train_df = pd.read_csv(train_df_path) \n",
        "class_names = [x for x in train_df[\"class_names\"].unique()]\n",
        "num_class = train_df[\"class\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkuuezStyCg-"
      },
      "outputs": [],
      "source": [
        "class CarsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file:str, root_dir:str, transform=None, custom_crop=False)->Dict:\n",
        "        \"\"\" Initializes the CarsDataset with the necessary variables\n",
        "        Args:\n",
        "            csv_file (str): Path to a CSV file containing the image paths and labels.\n",
        "            root_dir (str): Root directory where the images are stored.\n",
        "            transform (callable, optional): A function/transform that takes in a PIL image\n",
        "                and returns a transformed version. Default: None.\n",
        "            custom_crop (bool, optional): Whether to crop the images based on bounding box\n",
        "                coordinates specified in the CSV file. Default: False.\n",
        "        \"\"\"\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.custom_crop = custom_crop\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Returns the number of samples in the dataset.\n",
        "        Returns:\n",
        "            int: The number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx:str):\n",
        "        \"\"\" Loads and preprocesses the image and label at the specified index.\n",
        "            Provides an avenue to perform transformations on the dataset.\n",
        "        Args:\n",
        "            idx (int): The index of the sample to load.\n",
        "        Returns:\n",
        "            tuple: A tuple containing the preprocessed image tensor and its corresponding label tensor.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.dataframe.image[idx])\n",
        "        img = Image.open(img_name).convert(\"RGB\")\n",
        "        label = self.dataframe[\"class\"][idx]\n",
        "        left, top, right, bottom = self.dataframe.x1[idx], self.dataframe.y1[idx], self.dataframe.x2[idx], self.dataframe.y2[idx]\n",
        "        \n",
        "        if self.custom_crop:\n",
        "            img = self._custom_crop(img, left, top, right, bottom)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, torch.tensor(int(label))\n",
        "\n",
        "    def _custom_crop(self, img:Image, left:str, top:str, right:str, bottom:str)->Image:\n",
        "        \"\"\" Crops the image based on bounding box coordinates.\n",
        "        Args:\n",
        "            img (PIL.Image): The image to crop.\n",
        "            left (int): The left coordinate of the bounding box.\n",
        "            top (int): The top coordinate of the bounding box.\n",
        "            right (int): The right coordinate of the bounding box.\n",
        "            bottom (int): The bottom coordinate of the bounding box.\n",
        "        Returns:\n",
        "            PIL.Image: The cropped image.\n",
        "        \"\"\"\n",
        "        width = right-left\n",
        "        height = bottom-top\n",
        "        img = TF.crop(img, top, left, height, width)\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ErAdfAp_k32M"
      },
      "outputs": [],
      "source": [
        "# normalize mean and std from imagenet pretrained\n",
        "\n",
        "# Set inital parameters of the training loop\n",
        "epochs = cfg[\"training\"][\"epochs\"]\n",
        "batch_size = cfg[\"training\"][\"batch_size\"]\n",
        "custom_crop = cfg[\"training\"][\"custom_crop\"]\n",
        "# mean and standard dev as per pre-trained imagenet dataset (https://pytorch.org/hub/pytorch_vision_resnet/)\n",
        "mean = [0.485, 0.456, 0.406] \n",
        "std = [0.229, 0.224, 0.225]\n",
        "transform = T.Compose([\n",
        "                T.Resize([224,224]),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=mean, std=std),\n",
        "                ])\n",
        "train_num = int(round(0.8*len(train_df)))\n",
        "valid_num = int(round(0.2*len(train_df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset for training run\n",
        "path_to_data = os.path.join(os.getcwd(), \"data/car_ims\")\n",
        "dataset = CarsDataset(csv_file=train_df_path, root_dir=path_to_data, transform=transform, custom_crop=custom_crop)\n",
        "\n",
        "# Split dataset into training and validation runs \n",
        "train_set, valid_set = torch.utils.data.random_split(dataset, [train_num, valid_num])\n",
        "\n",
        "# Load the datasets into train and validation loaders respectively\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=4, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fem9ZTcALb5"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-e9lj4uUk32N"
      },
      "outputs": [],
      "source": [
        "class ClassifierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_class:int):\n",
        "        \"\"\" Initializes the ClassifierModel instance\n",
        "            Added a dropout to the last linear layer and amended out_features to num_class\n",
        "        Args:\n",
        "            num_class (int): The number of classes in the classification problem.\n",
        "        \"\"\"        \n",
        "        super().__init__()\n",
        "        self.cfg = read_yaml_file()\n",
        "\n",
        "        if self.cfg[\"training\"][\"model_name\"] == \"resnet101\":\n",
        "            model_func = resnet101\n",
        "            weight = ResNet101_Weights.IMAGENET1K_V2\n",
        "        elif self.cfg[\"training\"][\"model_name\"] == \"resnet50\":\n",
        "            model_func = resnet50\n",
        "            weight = ResNet50_Weights.IMAGENET1K_V2\n",
        "        else:\n",
        "            print(f\"Model not found, please ensure model loaded in model.py file\")\n",
        "\n",
        "        if self.cfg[\"inference\"][\"run_inference\"]==True:\n",
        "            self.model = model_func()\n",
        "        else:\n",
        "            self.model = model_func(weights=weight)\n",
        "\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, num_class))\n",
        "\n",
        "    def forward(self, x:torch.Tensor)-> torch.Tensor:\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmsiiN0Ok32O",
        "outputId": "f295c378-32f9-423c-a25c-c9e154a40caf"
      },
      "outputs": [],
      "source": [
        "# Instantiate Model\n",
        "model = ClassifierModel(num_class)\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXuCqLR4e-Bp",
        "outputId": "1e798dbc-8b99-4c5d-a536-fe41da861236"
      },
      "outputs": [],
      "source": [
        "# Instantiate tensorboard for metrics tracking\n",
        "writer = SummaryWriter()\n",
        "\n",
        "max_valid_acc=0.0\n",
        "for e in range(epochs):\n",
        "    \n",
        "    # Training Loop\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for data, labels in tqdm(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        target = model(data)\n",
        "        loss = loss_fn(target,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    training_loss = train_loss/len(train_loader)\n",
        "    writer.add_scalar(\"Loss/Train\", training_loss, e)\n",
        "    \n",
        "    print(f'Epoch {e+1} | Training Loss: {training_loss:.4f}')\n",
        "\n",
        "    # Validation Loop\n",
        "    valid_loss = 0.0\n",
        "    valid_correct = 0\n",
        "    model.eval()    \n",
        "    for batch_idx, (data, labels) in enumerate(tqdm(valid_loader)):\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "        \n",
        "        target = model(data)\n",
        "        loss = loss_fn(target,labels)\n",
        "        valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "        # Storing of grid images in tensorboard,\n",
        "        # Commented out for faster training times\n",
        "        # grid = make_grid(data)\n",
        "        # writer.add_image(\"images\", grid)\n",
        "        # writer.add_graph(model, data)\n",
        "\n",
        "        _, predicted = torch.max(target.detach(), 1)\n",
        "        valid_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_loss = valid_loss/len(valid_loader)\n",
        "    valid_acc = (valid_correct / len(valid_loader.dataset)) * 100\n",
        "    writer.add_scalar(\"Correct/Val\", valid_correct ,e)\n",
        "    writer.add_scalar(\"Loss/Val\", valid_loss, e)\n",
        "    writer.add_scalar(\"Acc/Val\", valid_acc, e)\n",
        "\n",
        "    print(f'Epoch {e} | Validation Loss: {valid_loss:.4f} | Validation Accuracy: {valid_acc:.2f}%')\n",
        "    \n",
        "    # Save out the models only if its accuracy exceed config amount \n",
        "    # and does better than the highest accuracy so far\n",
        "    save_above = cfg[\"training\"][\"save_above\"]\n",
        "    if (valid_acc>save_above) and (valid_acc>max_valid_acc):\n",
        "        max_valid_acc = valid_acc\n",
        "        \n",
        "        model_name = cfg[\"training\"][\"model_name\"]\n",
        "        if custom_crop:\n",
        "            name = f\"{model_name}_{valid_acc:.2f}_crop\"\n",
        "        else:\n",
        "            name = f\"{model_name}_{valid_acc:.2f}_nocrop\"\n",
        "        \n",
        "        model_path = os.path.join(os.getcwd(), f\"models/{name}.pth\") \n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "        # Log out each layer of the model per epoch\n",
        "        for name, weight in model.named_parameters():\n",
        "            writer.add_histogram(name,weight, e)\n",
        "            writer.add_histogram(f'{name}.grad',weight.grad, e)\n",
        "\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
